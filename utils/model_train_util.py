class TrainConfig:
    MODEL_ID = "TheBloke/zephyr-7B-beta-GGUF"
    # MODEL_ID = "HuggingFaceH4/zephyr-7b-beta"
    MODEL_FILE = "zephyr-7b-beta.Q5_K_M.gguf"
    MODEL_TYPE = "mistral"
    DATASET_ID = "bitext/Bitext-customer-support-llm-chatbot-training-dataset"
    CONTEXT_FIELD= ""
    INSTRUCTION_FIELD = "instruction"
    TARGET_FIELD = "response"
    BITS = 8
    DISABLE_EXLLAMA = True
    DEVICE_MAP = "auto"
    USE_CACHE = False
    LORA_R = 16
    LORA_ALPHA = 16
    LORA_DROPOUT = 0.05
    BIAS = "none"
    TARGET_MODULES = ["q_proj", "v_proj"]
    TASK_TYPE = "CAUSAL_LM"
    OUTPUT_DIR = "OpsHarmonySentinel"
    BATCH_SIZE = 8
    GRAD_ACCUMULATION_STEPS = 1
    OPTIMIZER = "paged_adamw_32bit"
    LR = 2e-4
    LR_SCHEDULER = "cosine"
    LOGGING_STEPS = 50
    SAVE_STRATEGY = "epoch"
    NUM_TRAIN_EPOCHS = 3
    # MAX_STEPS = 250
    FP16 = True
    PUSH_TO_HUB = False
    DATASET_TEXT_FIELD = "text"
    MAX_SEQ_LENGTH = 512
    PACKING = False