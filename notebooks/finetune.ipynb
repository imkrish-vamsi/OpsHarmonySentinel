{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q bitsandbytes datasets accelerate loralib\n",
    "! pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
    "! pip install optimum auto-gptq\n",
    "! pip install seaborn\n",
    "! pip install wandb\n",
    "! pip install flash-attn --no-build-isolation\n",
    "! pip install scipy ipywidgets\n",
    "! pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from random import shuffle\n",
    "import transformers, torch\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights And Biases Login\n",
    "wandb.login()\n",
    "wandb.init(project=\"OpsHarmonySentinel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR_PATH = ''\n",
    "OUTPUT_PATH = os.path.join(PROJECT_DIR_PATH, 'model', 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "BASELINE_MODEL_NAME = 'upstage/SOLAR-10.7B-Instruct-v1.0'\n",
    "# BASELINE_MODEL_NAME = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "PROJECT_NAME = 'OpsHarmonySentinel'\n",
    "HUGGING_FACE_REPO_NAME = f'imTheGodFather/{PROJECT_NAME}'\n",
    "HUGGING_FACE_MERGED_REPO_NAME = f'{HUGGING_FACE_REPO_NAME}'\n",
    "HUGGING_FACE_GPTQ_REPO_NAME = f'{HUGGING_FACE_REPO_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prepare Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/paperspace/documents/OpsHarmonySentinel/data/files/train.csv\"\n",
    "df = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_dialogue(text):\n",
    "    return [sentence.replace('User:', '').replace('Chip:', '').strip() for sentence in text.split('Assistant:')]\n",
    "\n",
    "def dialogue_to_chat(dialogue):\n",
    "    out = [{'role': 'system', 'content': 'You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains.'}]\n",
    "    # for idx, message in enumerate(dialogue):\n",
    "    #     role = 'user' if idx%2==0 else 'assistant'\n",
    "    #     out.append({'role': role, 'content': message})\n",
    "    input = f\"Investigate and provide root cause for the following incident - {dialogue['incident']}\"\n",
    "    out.append({'role': 'user', 'content': input})\n",
    "    out.append({'role': 'assistant', 'content': dialogue['root_cause']})\n",
    "    return out\n",
    "\n",
    "def chat_to_input(chat):\n",
    "    return tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "def process_example(example):\n",
    "    # out = text_to_dialogue(example)\n",
    "    out = dialogue_to_chat(example)\n",
    "    out = chat_to_input(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [process_example(row) for idx, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System:\n",
      "You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains.\n",
      "\n",
      "### User:\n",
      "Investigate and provide root cause for the following incident - [\n",
      "    {\n",
      "        \"incidentId\": \"E-99-9-999-9999999999\",\n",
      "        \"incidentStartTime\": \"04/01/2023 08:30\",\n",
      "        \"probabilityScore\": 0.90,\n",
      "        \"anomalyId\": \"AE-99-9999-9-F-S-ALL-99999999\",\n",
      "        \"anomalyTimestamp\": \"04/01/2023 08:30\",\n",
      "        \"applicationId\": \"affected-app\",\n",
      "        \"instanceId\": \"linux-server-42\",\n",
      "        \"serviceId\": \"affected-service\",\n",
      "        \"kpi\": \"INODES_USAGE\",\n",
      "        \"value\": 100.0,\n",
      "        \"thresholds\": {\"Upper\": 99.0, \"Lower\": 0.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"Inodes\"},\n",
      "            {\"kpiCategory\": \"FileSystem\"},\n",
      "            {\"kpiType\": \"Count\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"CRITICAL\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"Linux\"},\n",
      "            {\"componentName\": \"File System\"},\n",
      "            {\"componentType\": \"Inodes\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-99-9999-2-F-S-ALL-99999998\",\n",
      "        \"anomalyTimestamp\": \"04/01/2023 08:32\",\n",
      "        \"applicationId\": \"affected-app\",\n",
      "        \"instanceId\": \"linux-server-42\",\n",
      "        \"serviceId\": \"logging-service\",\n",
      "        \"kpi\": \"LOG_WRITE_FAILURES\",\n",
      "        \"value\": 50,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 0.0},\n",
      "        \"violationType\": \"Non-zero\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"WriteFailures\"},\n",
      "            {\"kpiCategory\": \"FileSystem\"},\n",
      "            {\"kpiType\": \"Count\"},\n",
      "            {\"anomalyLevel\": \"SERVICE\"},\n",
      "            {\"severity\": \"HIGH\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"Linux\"},\n",
      "            {\"componentName\": \"Logging Service\"},\n",
      "            {\"componentType\": \"Write Operations\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-99-9999-3-F-S-ALL-99999997\",\n",
      "        \"anomalyTimestamp\": \"04/01/2023 08:33\",\n",
      "        \"applicationId\": \"affected-app\",\n",
      "        \"instanceId\": \"linux-server-42\",\n",
      "        \"serviceId\": \"temp-file-creation-service\",\n",
      "        \"kpi\": \"TEMP_FILE_CREATION_FAILURES\",\n",
      "        \"value\": 30,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 0.0},\n",
      "        \"violationType\": \"Non-zero\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"FileCreationFailures\"},\n",
      "            {\"kpiCategory\": \"FileSystem\"},\n",
      "            {\"kpiType\": \"Count\"},\n",
      "            {\"anomalyLevel\": \"SERVICE\"},\n",
      "            {\"severity\": \"HIGH\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"Linux\"},\n",
      "            {\"componentName\": \"Temporary File Service\"},\n",
      "            {\"componentType\": \"File Creation\"}\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "\n",
      "### Assistant:\n",
      "The root cause could be due to the Linux server running out of inodes, which has prevented new files from being created on the server, thereby causing failures in services that require writing logs or temporary files for normal operation.\n"
     ]
    }
   ],
   "source": [
    "print(data[69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = list(map(tokenizer, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f0d64bc9be0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot sequence Lengths\n",
    "input_lengths = [len(x['input_ids']) for x in tokenized_data]\n",
    "sns.displot(input_lengths, kde=True).set(title='Distribution of Text Sequence Lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "split_idx = int(.99 * len(data))\n",
    "train_data, val_data = tokenized_data[:split_idx], tokenized_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Finetune a Quantized model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quantized model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                          load_in_8bit=True,     # call for the 8 bit bnb quantized version\n",
    "                                                          device_map='auto'\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PEFT adapter config (16:32)\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],                             # Apply to \"q_proj\", \"v_proj\" layers of attention (as suggested by paper)\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stabilize output layer and layernorms\n",
    "model = prepare_model_for_kbit_training(model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PEFT adapter on model (Last step)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "MAXLEN=512\n",
    "BATCH_SIZE=6\n",
    "GRAD_ACC=4\n",
    "WARMUP=100\n",
    "STEPS=50\n",
    "OPTIMIZER='paged_adamw_8bit' # save memory\n",
    "LR=4e-5                      # slightly smaller than pretraining lr | and close to LoRA standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Callbacks\n",
    "early_stop = transformers.EarlyStoppingCallback(10, 1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training config\n",
    "training_config = transformers.TrainingArguments(per_device_train_batch_size=BATCH_SIZE,\n",
    "                                                 gradient_accumulation_steps=GRAD_ACC,\n",
    "                                                 warmup_steps=WARMUP,\n",
    "                                                 max_steps=STEPS,\n",
    "                                                 optim=OPTIMIZER,\n",
    "                                                 learning_rate=LR,\n",
    "                                                 fp16=True,            # consider compatibility when using bf16\n",
    "                                                 logging_steps=1,\n",
    "                                                 output_dir=OUTPUT_PATH,\n",
    "                                                 report_to='wandb',\n",
    "                                                 # earlyStopping callback requirements\n",
    "                                                 load_best_model_at_end=True,\n",
    "                                                 evaluation_strategy='steps',\n",
    "                                                 metric_for_best_model='eval_loss',\n",
    "                                                 greater_is_better=False,\n",
    "                                                 eval_steps=10,\n",
    "                                                 save_steps=10,\n",
    "                                                 save_total_limit=2,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "trainer = transformers.Trainer(model=model,\n",
    "                               train_dataset=train_data,\n",
    "                               eval_dataset=val_data,\n",
    "                               data_collator=data_collator,\n",
    "                               args=training_config,\n",
    "                               callbacks=[early_stop],\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_REPO_NAME = HUGGING_FACE_REPO_NAME + '_10.7B_I_alpha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imTheGodFather/OpsHarmonySentinel_10.7B_I_alpha'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(HUGGING_FACE_REPO_NAME)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Merge Adapters (from quantized model) to dequantized model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try Model with Lora adapter\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a friendly chatbot assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Hello, what are your limitations as a seven billion parameters nlp model ?\"},\n",
    "# ]\n",
    "#\n",
    "# gen_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to('cuda')\n",
    "# gen_output = model.generate(input_ids=gen_input, max_new_tokens=512, do_sample=True)\n",
    "# print(tokenizer.decode(gen_output[0], skip_special_tokens=True))\n",
    "# # timeit : 12.6 s  3.39 s per loop (mean  std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peft config\n",
    "from peft import PeftConfig\n",
    "config = PeftConfig.from_pretrained(HUGGING_FACE_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                          torch_dtype=torch.float16, # GPTQ quantization requires fp16\n",
    "                                                          return_dict=True,\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lora model\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, HUGGING_FACE_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_MERGED_REPO_NAME = HUGGING_FACE_MERGED_REPO_NAME + '_10.7B_I_alpha_merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imTheGodFather/OpsHarmonySentinel_10.7B_I_alpha_merged'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_MERGED_REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.push_to_hub(HUGGING_FACE_MERGED_REPO_NAME)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_MERGED_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Quantize Merged Model to GGUF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id=HUGGING_FACE_MERGED_REPO_NAME\n",
    "snapshot_download(repo_id=model_id, local_dir=\"OHS-hf\",\n",
    "                  local_dir_use_symlinks=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.cpp is required to quantize to GGUF\n",
    "! git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file OHS-hf/model-00001-of-00003.safetensors\n",
      "Loading model file OHS-hf/model-00001-of-00003.safetensors\n",
      "Loading model file OHS-hf/model-00002-of-00003.safetensors\n",
      "Loading model file OHS-hf/model-00003-of-00003.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('OHS-hf'))\n",
      "Loading vocab file 'OHS-hf/tokenizer.model', type 'spm'\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [1024, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
      "Writing ohs-7b-alpha_8q.gguf, format 7\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 2\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to True\n",
      "gguf: Setting chat_template to {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "llama.cpp/convert.py:98: RuntimeWarning: invalid value encountered in divide\n",
      "  qs = (blocks / d[:, None]).round()\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type Q8_0 | T+   7\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+   7\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+   7\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+   7\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   7\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+   7\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   7\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   7\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+   7\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  10\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  11\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  12\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  12\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  12\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  12\n",
      "[ 20/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  13\n",
      "[ 21/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  14\n",
      "[ 22/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  14\n",
      "[ 23/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  14\n",
      "[ 24/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  14\n",
      "[ 25/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  14\n",
      "[ 26/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  15\n",
      "[ 28/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  17\n",
      "[ 29/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  17\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
      "[ 31/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  18\n",
      "[ 32/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  18\n",
      "[ 33/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  18\n",
      "[ 34/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  18\n",
      "[ 35/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  19\n",
      "[ 37/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  20\n",
      "[ 38/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  21\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  21\n",
      "[ 40/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  21\n",
      "[ 41/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  21\n",
      "[ 42/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  21\n",
      "[ 43/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  21\n",
      "[ 44/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  21\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  23\n",
      "[ 46/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  24\n",
      "[ 47/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  25\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  25\n",
      "[ 49/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[ 50/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 51/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 52/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[ 53/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  25\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  27\n",
      "[ 55/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  28\n",
      "[ 56/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  29\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 58/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  29\n",
      "[ 59/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 60/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 61/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  29\n",
      "[ 62/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  31\n",
      "[ 64/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  32\n",
      "[ 65/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  33\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  33\n",
      "[ 67/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  33\n",
      "[ 68/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  33\n",
      "[ 69/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  33\n",
      "[ 70/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  33\n",
      "[ 71/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  33\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  35\n",
      "[ 73/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  36\n",
      "[ 74/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  37\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  37\n",
      "[ 76/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  37\n",
      "[ 77/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[ 78/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[ 79/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  37\n",
      "[ 80/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  37\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  39\n",
      "[ 82/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  40\n",
      "[ 83/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  41\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  41\n",
      "[ 85/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  41\n",
      "[ 86/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[ 87/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[ 88/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  41\n",
      "[ 89/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  41\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  43\n",
      "[ 91/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  44\n",
      "[ 92/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  44\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  44\n",
      "[ 94/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  45\n",
      "[ 95/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  45\n",
      "[ 96/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  45\n",
      "[ 97/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  45\n",
      "[ 98/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  45\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  47\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  47\n",
      "[101/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  47\n",
      "[102/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  48\n",
      "[103/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  48\n",
      "[104/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  49\n",
      "[105/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  50\n",
      "[106/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  50\n",
      "[107/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  50\n",
      "[108/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  50\n",
      "[109/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  50\n",
      "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  50\n",
      "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  51\n",
      "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  52\n",
      "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  53\n",
      "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  53\n",
      "[115/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  53\n",
      "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  53\n",
      "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  53\n",
      "[118/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  53\n",
      "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  53\n",
      "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  55\n",
      "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  56\n",
      "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  57\n",
      "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  57\n",
      "[124/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  57\n",
      "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  57\n",
      "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  57\n",
      "[127/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  57\n",
      "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  57\n",
      "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  59\n",
      "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  60\n",
      "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  61\n",
      "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  61\n",
      "[133/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[136/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  61\n",
      "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  63\n",
      "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  64\n",
      "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  64\n",
      "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  65\n",
      "[142/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  65\n",
      "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[145/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  65\n",
      "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  65\n",
      "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  67\n",
      "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  68\n",
      "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  68\n",
      "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  69\n",
      "[151/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  69\n",
      "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[154/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  69\n",
      "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  69\n",
      "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  71\n",
      "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  72\n",
      "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  73\n",
      "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  73\n",
      "[160/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  73\n",
      "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  73\n",
      "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  73\n",
      "[163/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  73\n",
      "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  73\n",
      "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  75\n",
      "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  76\n",
      "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  77\n",
      "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  77\n",
      "[169/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  77\n",
      "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  77\n",
      "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  77\n",
      "[172/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  77\n",
      "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  77\n",
      "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  79\n",
      "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  80\n",
      "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  81\n",
      "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  81\n",
      "[178/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  81\n",
      "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  81\n",
      "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  81\n",
      "[181/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  81\n",
      "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  81\n",
      "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  83\n",
      "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  84\n",
      "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  85\n",
      "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  85\n",
      "[187/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  85\n",
      "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  85\n",
      "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  85\n",
      "[190/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  85\n",
      "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  85\n",
      "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  88\n",
      "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  89\n",
      "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  89\n",
      "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  89\n",
      "[196/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  89\n",
      "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  89\n",
      "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  89\n",
      "[199/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  89\n",
      "[200/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  89\n",
      "[201/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  90\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  91\n",
      "[203/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  91\n",
      "[204/291] Writing tensor output.weight                          | size  32000 x   4096  | type Q8_0 | T+  97\n",
      "[205/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  97\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  97\n",
      "[207/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  97\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  98\n",
      "[209/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  98\n",
      "[210/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  98\n",
      "[211/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  98\n",
      "[212/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 100\n",
      "[213/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 101\n",
      "[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 102\n",
      "[215/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 102\n",
      "[216/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 102\n",
      "[217/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 102\n",
      "[218/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 102\n",
      "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 102\n",
      "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 104\n",
      "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 104\n",
      "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 105\n",
      "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 106\n",
      "[224/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 106\n",
      "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 106\n",
      "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 106\n",
      "[227/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 106\n",
      "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 106\n",
      "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 108\n",
      "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 108\n",
      "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 109\n",
      "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 110\n",
      "[233/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 110\n",
      "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 110\n",
      "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 110\n",
      "[236/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 110\n",
      "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 110\n",
      "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 112\n",
      "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 113\n",
      "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 114\n",
      "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 114\n",
      "[242/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 114\n",
      "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 114\n",
      "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 114\n",
      "[245/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 114\n",
      "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 114\n",
      "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 116\n",
      "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 117\n",
      "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 118\n",
      "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 118\n",
      "[251/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 118\n",
      "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 118\n",
      "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 118\n",
      "[254/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 118\n",
      "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 118\n",
      "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 121\n",
      "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 122\n",
      "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 122\n",
      "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 122\n",
      "[260/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 122\n",
      "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 122\n",
      "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 122\n",
      "[263/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 122\n",
      "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 122\n",
      "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 125\n",
      "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 126\n",
      "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 127\n",
      "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 127\n",
      "[269/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 127\n",
      "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 127\n",
      "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 127\n",
      "[272/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 127\n",
      "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 127\n",
      "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 129\n",
      "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 131\n",
      "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 131\n",
      "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 131\n",
      "[278/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 131\n",
      "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 131\n",
      "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 131\n",
      "[281/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 131\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 131\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 134\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 134\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 135\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 135\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 135\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 135\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 135\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 135\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 135\n",
      "Wrote ohs-7b-alpha_8q.gguf\n"
     ]
    }
   ],
   "source": [
    "! python llama.cpp/convert.py OHS-hf \\\n",
    "  --outfile ohs-7b-alpha_8q.gguf \\\n",
    "  --outtype q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Quantize Merged Model using GPTQ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFLOAD_PATH = os.path.join(PROJECT_DIR_PATH, 'offload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(HUGGING_FACE_REPO_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = transformers.GPTQConfig(bits=8,\n",
    "                                            #   group_size=128,\n",
    "                                            #   desc_act=False,\n",
    "                                              dataset=data[:660],\n",
    "                                              tokenizer=tokenizer,\n",
    "                                              use_exllama=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model\n",
    "q_model = transformers.AutoModelForCausalLM.from_pretrained(HUGGING_FACE_MERGED_REPO_NAME,\n",
    "                                                            quantization_config=quantization_config,\n",
    "                                                            # torch_dtype=torch.float16,\n",
    "                                                            device_map=\"auto\",\n",
    "                                                            # offload_folder=OFFLOAD_PATH\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_GPTQ_REPO_NAME = HUGGING_FACE_GPTQ_REPO_NAME + '_10.7B_I_alpha_8Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imTheGodFather/OpsHarmonySentinel_10.7B_I_alpha_8Q'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_GPTQ_REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HF hub\n",
    "q_model.push_to_hub(HUGGING_FACE_GPTQ_REPO_NAME)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_GPTQ_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inference Using GPTQ models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Analyse and provide root cause for the following ITOps incident. Use the forensics to support \n",
    "your answer. You should be technically accurate and up to the point.\n",
    "[\n",
    "    {\n",
    "        \"incidentId\": \"E-12-1-635-1700153820\",\n",
    "        \"incidentStartTime\": \"11/16/2023 16:57\",\n",
    "        \"probabilityScore\": 0.82,\n",
    "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335897\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"instanceId\": \"mysql-percona-153\",\n",
    "        \"serviceId\": \"mysql-percona-svc\",\n",
    "        \"kpi\": \"CPU_UTIL\",\n",
    "        \"value\": 98.44,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"CPU\"},\n",
    "            {\"kpiCategory\": \"CPU\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"INSTANCE\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"MySql\"},\n",
    "            {\"componentVersion\": \"8.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335899\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:59\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"instanceId\": \"mysql-percona-153\",\n",
    "        \"serviceId\": \"mysql-percona-svc\",\n",
    "        \"kpi\": \"CPU_UTIL\",\n",
    "        \"value\": 99.2,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"CPU\"},\n",
    "            {\"kpiCategory\": \"CPU\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"INSTANCE\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"MySql\"},\n",
    "            {\"componentVersion\": \"8.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-6224-6-C-S-ALL-28335898\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:58\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"instanceId\": \"mysql-percona-153\",\n",
    "        \"serviceId\": \"mysql-percona-svc\",\n",
    "        \"kpi\": \"LOAD_AVG\",\n",
    "        \"value\": 11.21,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 8.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"LoadAvg\"},\n",
    "            {\"kpiCategory\": \"CPU\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"INSTANCE\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"MySql\"},\n",
    "            {\"componentVersion\": \"8.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-1031-397-T-S-28335897\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"serviceId\": \"shopping-product-service\",\n",
    "        \"kpi\": \"RESPONSE_TIME\",\n",
    "        \"value\": 113.98,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"RESPONSE TIME\"},\n",
    "            {\"kpiCategory\": \"Workload\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"CLUSTER\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"Springboot Micro-service\"},\n",
    "            {\"javaVersion\": \"8.0\"}\n",
    "        ],\n",
    "        \"forensics\": \"CPU Utilization: 98.94%\n",
    "        Number of CPU cores: 8\n",
    "        Note: Total of CPU Utilization depends on number of CPU cores and can be > 100%\n",
    "        -----\n",
    "        PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n",
    "        11629 root      20   0   16160    236     20 R  94.1  0.0   4:27.17 batchjob\n",
    "        14602 heal      20   0 5119288 835492  16448 S  11.8  5.1 398:57.59 mysqld\n",
    "        \"\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-1031-397-T-S-28335899\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"serviceId\": \"shopping-product-service\",\n",
    "        \"kpi\": \"RESPONSE_TIME\",\n",
    "        \"value\": 117.92,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"RESPONSE TIME\"},\n",
    "            {\"kpiCategory\": \"Workload\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"CLUSTER\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"Springboot Micro-service\"},\n",
    "            {\"javaVersion\": \"8.0\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(HUGGING_FACE_GPTQ_REPO_NAME,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                          # torch_dtype=torch.bfloat16,\n",
    "                                                          # use_flash_attention_2=True,\n",
    "                                                          # low_cpu_mem_usage=True,\n",
    "                                                         )\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(HUGGING_FACE_GPTQ_REPO_NAME, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System:\n",
      "You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains. You answer specifically only to the question asked.\n",
      "\n",
      "### User:\n",
      "Analyse and provide root cause for the following ITOps incident. Use the forensics to support \n",
      "your answer. You should be technically accurate and up to the point.\n",
      "[\n",
      "    {\n",
      "        \"incidentId\": \"E-12-1-635-1700153820\",\n",
      "        \"incidentStartTime\": \"11/16/2023 16:57\",\n",
      "        \"probabilityScore\": 0.82,\n",
      "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335897\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"instanceId\": \"mysql-percona-153\",\n",
      "        \"serviceId\": \"mysql-percona-svc\",\n",
      "        \"kpi\": \"CPU_UTIL\",\n",
      "        \"value\": 98.44,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"CPU\"},\n",
      "            {\"kpiCategory\": \"CPU\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"MySql\"},\n",
      "            {\"componentVersion\": \"8.0\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335899\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:59\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"instanceId\": \"mysql-percona-153\",\n",
      "        \"serviceId\": \"mysql-percona-svc\",\n",
      "        \"kpi\": \"CPU_UTIL\",\n",
      "        \"value\": 99.2,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"CPU\"},\n",
      "            {\"kpiCategory\": \"CPU\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"MySql\"},\n",
      "            {\"componentVersion\": \"8.0\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-6224-6-C-S-ALL-28335898\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:58\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"instanceId\": \"mysql-percona-153\",\n",
      "        \"serviceId\": \"mysql-percona-svc\",\n",
      "        \"kpi\": \"LOAD_AVG\",\n",
      "        \"value\": 11.21,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 8.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"LoadAvg\"},\n",
      "            {\"kpiCategory\": \"CPU\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"MySql\"},\n",
      "            {\"componentVersion\": \"8.0\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-1031-397-T-S-28335897\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"serviceId\": \"shopping-product-service\",\n",
      "        \"kpi\": \"RESPONSE_TIME\",\n",
      "        \"value\": 113.98,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"RESPONSE TIME\"},\n",
      "            {\"kpiCategory\": \"Workload\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"CLUSTER\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"Springboot Micro-service\"},\n",
      "            {\"javaVersion\": \"8.0\"}\n",
      "        ],\n",
      "        \"forensics\": \"CPU Utilization: 98.94%\n",
      "        Number of CPU cores: 8\n",
      "        Note: Total of CPU Utilization depends on number of CPU cores and can be > 100%\n",
      "        -----\n",
      "        PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n",
      "        11629 root      20   0   16160    236     20 R  94.1  0.0   4:27.17 batchjob\n",
      "        14602 heal      20   0 5119288 835492  16448 S  11.8  5.1 398:57.59 mysqld\n",
      "        \"\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-1031-397-T-S-28335899\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"serviceId\": \"shopping-product-service\",\n",
      "        \"kpi\": \"RESPONSE_TIME\",\n",
      "        \"value\": 117.92,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"RESPONSE TIME\"},\n",
      "            {\"kpiCategory\": \"Workload\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"CLUSTER\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"Springboot Micro-service\"},\n",
      "            {\"javaVersion\": \"8.0\"}\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "\n",
      "\n",
      "#### \n",
      "Analysis: The root cause for this incident appears to be high load and resource contention due to background tasks running on the MySQL server (mysql-percona-153). This is indicated by anomalies observed on the CPU utilization, load average, and response time metrics.\n",
      "\n",
      "The high CPU usage (98.44%, then 99.2%) along with the high load average (11.21) on the MySQL instance coincides with the significant increase in response time observed across the cluster for the Spring boot microservices. These metrics suggest that the MySQL server was heavily loaded, leading to increased wait times for queries from the application.\n",
      "\n",
      "The process output from the 'batchjob' service (PID: 11629) consuming nearly 94% CPU resources further supports the contention hypothesis. It could be an overdue database maintenance task or a data migration job, causing the excessive load on the MySQL server.\n",
      "\n",
      "In summary, the root cause for this incident is attributed to a high load on the MySQL server (mysql-percona-153) resulting from concurrently executing heavyweight background tasks such as the 'batchjob'. This led to resource contention and ultimately impacted the performance of the connected microservices.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains. You answer specifically only to the question asked.\"},\n",
    "    {\"role\": \"user\", \"content\": question},]\n",
    "gen_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "gen_output = model.generate(input_ids=gen_input,\n",
    "                            max_new_tokens=512,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7,\n",
    "                            top_k=50,\n",
    "                            top_p=0.95,\n",
    "                            repetition_penalty=1.1)\n",
    "out = tokenizer.decode(gen_output[0], skip_special_tokens=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inference Using GGUF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoConfig\n",
    "config = AutoConfig.from_pretrained(r\"/home/paperspace/documents/OpsHarmonySentinel/model/files/ohs-7b-alpha_8q.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.max_seq_len = 4096\n",
    "# config.max_answer_len= 1024\n",
    "config.config.max_new_tokens = 2048\n",
    "config.config.context_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(r\"/home/paperspace/documents/OpsHarmonySentinel/model/files/ohs-7b-alpha_8q.gguf\", model_type='mistral', gpu_layers=0, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the incident and forensics, we can conclude that the root cause of this ITOps incident is CPU contention due to high CPU utilization by the batchjob process (pid 11629) running on the same server as MySQL database (mysql-percona-153). This has resulted in high load average values for MySQL server and degraded performance, leading to increased response times for requests handled by the Springboot microservice (shopping-product-service). The forensics also indicate that there is no resource contention or bottleneck at the application layer.\n",
      "To mitigate this issue, we can consider options such as spreading out batch jobs across multiple servers with more resources or allocating dedicated CPU cores to critical workloads such as database and microservices. We may also need to optimize the performance of our current server by reducing unnecessary tasks during the execution of the batchjob process or considering tuning parameter such as kernel setting.\n"
     ]
    }
   ],
   "source": [
    "print(llm(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpsHarmonySentinel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
