{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q bitsandbytes datasets accelerate loralib\n",
    "! pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
    "! pip install optimum auto-gptq\n",
    "! pip install seaborn\n",
    "! pip install wandb\n",
    "! pip install flash-attn --no-build-isolation\n",
    "! pip install scipy ipywidgets\n",
    "! pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from random import shuffle\n",
    "import transformers, torch\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights And Biases Login\n",
    "wandb.login()\n",
    "wandb.init(project=\"OpsHarmonySentinel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR_PATH = ''\n",
    "OUTPUT_PATH = os.path.join(PROJECT_DIR_PATH, 'model', 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "BASELINE_MODEL_NAME = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "PROJECT_NAME = 'OpsHarmonySentinel'\n",
    "HUGGING_FACE_REPO_NAME = f'imTheGodFather/{PROJECT_NAME}'\n",
    "HUGGING_FACE_MERGED_REPO_NAME = f'{HUGGING_FACE_REPO_NAME}'\n",
    "HUGGING_FACE_GPTQ_REPO_NAME = f'{HUGGING_FACE_REPO_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prepare Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/paperspace/documents/OpsHarmonySentinel/data/files/train.csv\"\n",
    "df = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_dialogue(text):\n",
    "    return [sentence.replace('User:', '').replace('Chip:', '').strip() for sentence in text.split('Assistant:')]\n",
    "\n",
    "def dialogue_to_chat(dialogue):\n",
    "    out = [{'role': 'system', 'content': 'You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains.'}]\n",
    "    # for idx, message in enumerate(dialogue):\n",
    "    #     role = 'user' if idx%2==0 else 'assistant'\n",
    "    #     out.append({'role': role, 'content': message})\n",
    "    input = f\"Investigate and provide root cause for the following incident - {dialogue['incident']}\"\n",
    "    out.append({'role': 'user', 'content': input})\n",
    "    out.append({'role': 'assistant', 'content': dialogue['root_cause']})\n",
    "    return out\n",
    "\n",
    "def chat_to_input(chat):\n",
    "    return tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "def process_example(example):\n",
    "    # out = text_to_dialogue(example)\n",
    "    out = dialogue_to_chat(example)\n",
    "    out = chat_to_input(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [process_example(row) for idx, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains.</s>\n",
      "<|user|>\n",
      "Investigate and provide root cause for the following incident - [\n",
      "    {\n",
      "        \"incidentId\": \"E-99-9-999-9999999999\",\n",
      "        \"incidentStartTime\": \"04/01/2023 08:30\",\n",
      "        \"probabilityScore\": 0.90,\n",
      "        \"anomalyId\": \"AE-99-9999-9-F-S-ALL-99999999\",\n",
      "        \"anomalyTimestamp\": \"04/01/2023 08:30\",\n",
      "        \"applicationId\": \"affected-app\",\n",
      "        \"instanceId\": \"linux-server-42\",\n",
      "        \"serviceId\": \"affected-service\",\n",
      "        \"kpi\": \"INODES_USAGE\",\n",
      "        \"value\": 100.0,\n",
      "        \"thresholds\": {\"Upper\": 99.0, \"Lower\": 0.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"Inodes\"},\n",
      "            {\"kpiCategory\": \"FileSystem\"},\n",
      "            {\"kpiType\": \"Count\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"CRITICAL\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"Linux\"},\n",
      "            {\"componentName\": \"File System\"},\n",
      "            {\"componentType\": \"Inodes\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-99-9999-2-F-S-ALL-99999998\",\n",
      "        \"anomalyTimestamp\": \"04/01/2023 08:32\",\n",
      "        \"applicationId\": \"affected-app\",\n",
      "        \"instanceId\": \"linux-server-42\",\n",
      "        \"serviceId\": \"logging-service\",\n",
      "        \"kpi\": \"LOG_WRITE_FAILURES\",\n",
      "        \"value\": 50,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 0.0},\n",
      "        \"violationType\": \"Non-zero\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"WriteFailures\"},\n",
      "            {\"kpiCategory\": \"FileSystem\"},\n",
      "            {\"kpiType\": \"Count\"},\n",
      "            {\"anomalyLevel\": \"SERVICE\"},\n",
      "            {\"severity\": \"HIGH\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"Linux\"},\n",
      "            {\"componentName\": \"Logging Service\"},\n",
      "            {\"componentType\": \"Write Operations\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-99-9999-3-F-S-ALL-99999997\",\n",
      "        \"anomalyTimestamp\": \"04/01/2023 08:33\",\n",
      "        \"applicationId\": \"affected-app\",\n",
      "        \"instanceId\": \"linux-server-42\",\n",
      "        \"serviceId\": \"temp-file-creation-service\",\n",
      "        \"kpi\": \"TEMP_FILE_CREATION_FAILURES\",\n",
      "        \"value\": 30,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 0.0},\n",
      "        \"violationType\": \"Non-zero\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"FileCreationFailures\"},\n",
      "            {\"kpiCategory\": \"FileSystem\"},\n",
      "            {\"kpiType\": \"Count\"},\n",
      "            {\"anomalyLevel\": \"SERVICE\"},\n",
      "            {\"severity\": \"HIGH\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"Linux\"},\n",
      "            {\"componentName\": \"Temporary File Service\"},\n",
      "            {\"componentType\": \"File Creation\"}\n",
      "        ]\n",
      "    }\n",
      "]</s>\n",
      "<|assistant|>\n",
      "The root cause could be due to the Linux server running out of inodes, which has prevented new files from being created on the server, thereby causing failures in services that require writing logs or temporary files for normal operation.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = list(map(tokenizer, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f2623be2820>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot sequence Lengths\n",
    "input_lengths = [len(x['input_ids']) for x in tokenized_data]\n",
    "sns.displot(input_lengths, kde=True).set(title='Distribution of Text Sequence Lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "split_idx = int(.99 * len(data))\n",
    "train_data, val_data = tokenized_data[:split_idx], tokenized_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Finetune a Quantized model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quantized model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                          load_in_8bit=True,     # call for the 8 bit bnb quantized version\n",
    "                                                          device_map='auto'\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PEFT adapter config (16:32)\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],                             # Apply to \"q_proj\", \"v_proj\" layers of attention (as suggested by paper)\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stabilize output layer and layernorms\n",
    "model = prepare_model_for_kbit_training(model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PEFT adapter on model (Last step)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "MAXLEN=512\n",
    "BATCH_SIZE=6\n",
    "GRAD_ACC=4\n",
    "WARMUP=100\n",
    "STEPS=50\n",
    "OPTIMIZER='paged_adamw_8bit' # save memory\n",
    "LR=4e-5                      # slightly smaller than pretraining lr | and close to LoRA standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Callbacks\n",
    "early_stop = transformers.EarlyStoppingCallback(10, 1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training config\n",
    "training_config = transformers.TrainingArguments(per_device_train_batch_size=BATCH_SIZE,\n",
    "                                                 gradient_accumulation_steps=GRAD_ACC,\n",
    "                                                 warmup_steps=WARMUP,\n",
    "                                                 max_steps=STEPS,\n",
    "                                                 optim=OPTIMIZER,\n",
    "                                                 learning_rate=LR,\n",
    "                                                 fp16=True,            # consider compatibility when using bf16\n",
    "                                                 logging_steps=1,\n",
    "                                                 output_dir=OUTPUT_PATH,\n",
    "                                                 report_to='wandb',\n",
    "                                                 # earlyStopping callback requirements\n",
    "                                                 load_best_model_at_end=True,\n",
    "                                                 evaluation_strategy='steps',\n",
    "                                                 metric_for_best_model='eval_loss',\n",
    "                                                 greater_is_better=False,\n",
    "                                                 eval_steps=10,\n",
    "                                                 save_steps=10,\n",
    "                                                 save_total_limit=2,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "trainer = transformers.Trainer(model=model,\n",
    "                               train_dataset=train_data,\n",
    "                               eval_dataset=val_data,\n",
    "                               data_collator=data_collator,\n",
    "                               args=training_config,\n",
    "                               callbacks=[early_stop],\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 44:17, Epoch 11/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.811600</td>\n",
       "      <td>0.900489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.853900</td>\n",
       "      <td>0.874403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.780600</td>\n",
       "      <td>0.829738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.764200</td>\n",
       "      <td>0.759371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.669697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.7833893740177155, metrics={'train_runtime': 2710.2188, 'train_samples_per_second': 0.443, 'train_steps_per_second': 0.018, 'total_flos': 6.879137197031424e+16, 'train_loss': 0.7833893740177155, 'epoch': 11.76})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_REPO_NAME = HUGGING_FACE_REPO_NAME + '_7B_alpha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imTheGodFather/OpsHarmonySentinel_7B_alpha'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(HUGGING_FACE_REPO_NAME)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Merge Adapters (from quantized model) to dequantized model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try Model with Lora adapter\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a friendly chatbot assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Hello, what are your limitations as a seven billion parameters nlp model ?\"},\n",
    "# ]\n",
    "#\n",
    "# gen_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to('cuda')\n",
    "# gen_output = model.generate(input_ids=gen_input, max_new_tokens=512, do_sample=True)\n",
    "# print(tokenizer.decode(gen_output[0], skip_special_tokens=True))\n",
    "# # timeit : 12.6 s  3.39 s per loop (mean  std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peft config\n",
    "from peft import PeftConfig\n",
    "config = PeftConfig.from_pretrained(HUGGING_FACE_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                          torch_dtype=torch.float16, # GPTQ quantization requires fp16\n",
    "                                                          return_dict=True,\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lora model\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, HUGGING_FACE_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_MERGED_REPO_NAME = HUGGING_FACE_MERGED_REPO_NAME + '_7B_merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imTheGodFather/OpsHarmonySentinel_7B_merged'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_MERGED_REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.push_to_hub(HUGGING_FACE_MERGED_REPO_NAME)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_MERGED_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Quantize Merged Model to GGUF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id=HUGGING_FACE_MERGED_REPO_NAME\n",
    "snapshot_download(repo_id=model_id, local_dir=\"OHS-hf\",\n",
    "                  local_dir_use_symlinks=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.cpp is required to quantize to GGUF\n",
    "! git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file OHS-hf/model-00001-of-00003.safetensors\n",
      "Loading model file OHS-hf/model-00001-of-00003.safetensors\n",
      "Loading model file OHS-hf/model-00002-of-00003.safetensors\n",
      "Loading model file OHS-hf/model-00003-of-00003.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('OHS-hf'))\n",
      "Loading vocab file 'OHS-hf/tokenizer.model', type 'spm'\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [1024, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
      "Writing ohs-7b-alpha_8q.gguf, format 7\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 2\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to True\n",
      "gguf: Setting chat_template to {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "llama.cpp/convert.py:98: RuntimeWarning: invalid value encountered in divide\n",
      "  qs = (blocks / d[:, None]).round()\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type Q8_0 | T+   7\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+   7\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+   7\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+   7\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   7\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+   7\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   7\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   7\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+   7\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  10\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  11\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  12\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  12\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  12\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  12\n",
      "[ 20/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  13\n",
      "[ 21/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  14\n",
      "[ 22/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  14\n",
      "[ 23/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  14\n",
      "[ 24/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  14\n",
      "[ 25/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  14\n",
      "[ 26/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  15\n",
      "[ 28/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  17\n",
      "[ 29/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  17\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
      "[ 31/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  18\n",
      "[ 32/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  18\n",
      "[ 33/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  18\n",
      "[ 34/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  18\n",
      "[ 35/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  19\n",
      "[ 37/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  20\n",
      "[ 38/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  21\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  21\n",
      "[ 40/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  21\n",
      "[ 41/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  21\n",
      "[ 42/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  21\n",
      "[ 43/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  21\n",
      "[ 44/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  21\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  23\n",
      "[ 46/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  24\n",
      "[ 47/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  25\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  25\n",
      "[ 49/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[ 50/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 51/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 52/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[ 53/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  25\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  27\n",
      "[ 55/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  28\n",
      "[ 56/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  29\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 58/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  29\n",
      "[ 59/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 60/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 61/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  29\n",
      "[ 62/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  31\n",
      "[ 64/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  32\n",
      "[ 65/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  33\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  33\n",
      "[ 67/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  33\n",
      "[ 68/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  33\n",
      "[ 69/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  33\n",
      "[ 70/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  33\n",
      "[ 71/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  33\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  35\n",
      "[ 73/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  36\n",
      "[ 74/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  37\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  37\n",
      "[ 76/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  37\n",
      "[ 77/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[ 78/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[ 79/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  37\n",
      "[ 80/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  37\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  39\n",
      "[ 82/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  40\n",
      "[ 83/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  41\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  41\n",
      "[ 85/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  41\n",
      "[ 86/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[ 87/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[ 88/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  41\n",
      "[ 89/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  41\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  43\n",
      "[ 91/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  44\n",
      "[ 92/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  44\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  44\n",
      "[ 94/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  45\n",
      "[ 95/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  45\n",
      "[ 96/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  45\n",
      "[ 97/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  45\n",
      "[ 98/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  45\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  47\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  47\n",
      "[101/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  47\n",
      "[102/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  48\n",
      "[103/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  48\n",
      "[104/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  49\n",
      "[105/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  50\n",
      "[106/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  50\n",
      "[107/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  50\n",
      "[108/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  50\n",
      "[109/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  50\n",
      "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  50\n",
      "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  51\n",
      "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  52\n",
      "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  53\n",
      "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  53\n",
      "[115/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  53\n",
      "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  53\n",
      "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  53\n",
      "[118/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  53\n",
      "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  53\n",
      "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  55\n",
      "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  56\n",
      "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  57\n",
      "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  57\n",
      "[124/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  57\n",
      "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  57\n",
      "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  57\n",
      "[127/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  57\n",
      "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  57\n",
      "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  59\n",
      "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  60\n",
      "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  61\n",
      "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  61\n",
      "[133/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[136/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  61\n",
      "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  63\n",
      "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  64\n",
      "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  64\n",
      "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  65\n",
      "[142/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  65\n",
      "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[145/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  65\n",
      "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  65\n",
      "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  67\n",
      "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  68\n",
      "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  68\n",
      "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  69\n",
      "[151/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  69\n",
      "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[154/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  69\n",
      "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  69\n",
      "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  71\n",
      "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  72\n",
      "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  73\n",
      "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  73\n",
      "[160/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  73\n",
      "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  73\n",
      "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  73\n",
      "[163/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  73\n",
      "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  73\n",
      "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  75\n",
      "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  76\n",
      "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  77\n",
      "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  77\n",
      "[169/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  77\n",
      "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  77\n",
      "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  77\n",
      "[172/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  77\n",
      "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  77\n",
      "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  79\n",
      "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  80\n",
      "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  81\n",
      "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  81\n",
      "[178/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  81\n",
      "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  81\n",
      "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  81\n",
      "[181/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  81\n",
      "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  81\n",
      "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  83\n",
      "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  84\n",
      "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  85\n",
      "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  85\n",
      "[187/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  85\n",
      "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  85\n",
      "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  85\n",
      "[190/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  85\n",
      "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  85\n",
      "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  88\n",
      "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  89\n",
      "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  89\n",
      "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  89\n",
      "[196/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  89\n",
      "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  89\n",
      "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  89\n",
      "[199/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  89\n",
      "[200/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  89\n",
      "[201/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  90\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  91\n",
      "[203/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  91\n",
      "[204/291] Writing tensor output.weight                          | size  32000 x   4096  | type Q8_0 | T+  97\n",
      "[205/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  97\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  97\n",
      "[207/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  97\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  98\n",
      "[209/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  98\n",
      "[210/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  98\n",
      "[211/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  98\n",
      "[212/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 100\n",
      "[213/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 101\n",
      "[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 102\n",
      "[215/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 102\n",
      "[216/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 102\n",
      "[217/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 102\n",
      "[218/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 102\n",
      "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 102\n",
      "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 104\n",
      "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 104\n",
      "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 105\n",
      "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 106\n",
      "[224/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 106\n",
      "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 106\n",
      "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 106\n",
      "[227/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 106\n",
      "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 106\n",
      "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 108\n",
      "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 108\n",
      "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 109\n",
      "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 110\n",
      "[233/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 110\n",
      "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 110\n",
      "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 110\n",
      "[236/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 110\n",
      "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 110\n",
      "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 112\n",
      "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 113\n",
      "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 114\n",
      "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 114\n",
      "[242/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 114\n",
      "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 114\n",
      "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 114\n",
      "[245/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 114\n",
      "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 114\n",
      "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 116\n",
      "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 117\n",
      "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 118\n",
      "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 118\n",
      "[251/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 118\n",
      "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 118\n",
      "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 118\n",
      "[254/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 118\n",
      "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 118\n",
      "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 121\n",
      "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 122\n",
      "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 122\n",
      "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 122\n",
      "[260/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 122\n",
      "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 122\n",
      "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 122\n",
      "[263/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 122\n",
      "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 122\n",
      "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 125\n",
      "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 126\n",
      "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 127\n",
      "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 127\n",
      "[269/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 127\n",
      "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 127\n",
      "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 127\n",
      "[272/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 127\n",
      "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 127\n",
      "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 129\n",
      "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 131\n",
      "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 131\n",
      "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 131\n",
      "[278/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 131\n",
      "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 131\n",
      "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 131\n",
      "[281/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 131\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 131\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 134\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 134\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 135\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 135\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 135\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 135\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 135\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 135\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 135\n",
      "Wrote ohs-7b-alpha_8q.gguf\n"
     ]
    }
   ],
   "source": [
    "! python llama.cpp/convert.py OHS-hf \\\n",
    "  --outfile ohs-7b-alpha_8q.gguf \\\n",
    "  --outtype q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Quantize Merged Model using GPTQ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFLOAD_PATH = os.path.join(PROJECT_DIR_PATH, 'offload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(HUGGING_FACE_REPO_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = transformers.GPTQConfig(bits=4,\n",
    "                                            #   group_size=128,\n",
    "                                            #   desc_act=False,\n",
    "                                              dataset=data[:660],\n",
    "                                              tokenizer=tokenizer,\n",
    "                                              use_exllama=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model\n",
    "q_model = transformers.AutoModelForCausalLM.from_pretrained(HUGGING_FACE_MERGED_REPO_NAME,\n",
    "                                                            quantization_config=quantization_config,\n",
    "                                                            # torch_dtype=torch.float16,\n",
    "                                                            device_map=\"auto\",\n",
    "                                                            # offload_folder=OFFLOAD_PATH\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_GPTQ_REPO_NAME = HUGGING_FACE_GPTQ_REPO_NAME + '_7B_alpha_4Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imTheGodFather/OpsHarmonySentinel_7B_alpha_4Q'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_GPTQ_REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HF hub\n",
    "q_model.push_to_hub(HUGGING_FACE_GPTQ_REPO_NAME)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_GPTQ_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inference Using GPTQ models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Analyse and provide root cause for the following ITOps incident. Use the forensics to support \n",
    "your answer. You should be technically accurate and up to the point.\n",
    "[\n",
    "    {\n",
    "        \"incidentId\": \"E-12-1-635-1700153820\",\n",
    "        \"incidentStartTime\": \"11/16/2023 16:57\",\n",
    "        \"probabilityScore\": 0.82,\n",
    "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335897\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"instanceId\": \"mysql-percona-153\",\n",
    "        \"serviceId\": \"mysql-percona-svc\",\n",
    "        \"kpi\": \"CPU_UTIL\",\n",
    "        \"value\": 98.44,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"CPU\"},\n",
    "            {\"kpiCategory\": \"CPU\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"INSTANCE\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"MySql\"},\n",
    "            {\"componentVersion\": \"8.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335899\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:59\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"instanceId\": \"mysql-percona-153\",\n",
    "        \"serviceId\": \"mysql-percona-svc\",\n",
    "        \"kpi\": \"CPU_UTIL\",\n",
    "        \"value\": 99.2,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"CPU\"},\n",
    "            {\"kpiCategory\": \"CPU\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"INSTANCE\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"MySql\"},\n",
    "            {\"componentVersion\": \"8.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-6224-6-C-S-ALL-28335898\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:58\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"instanceId\": \"mysql-percona-153\",\n",
    "        \"serviceId\": \"mysql-percona-svc\",\n",
    "        \"kpi\": \"LOAD_AVG\",\n",
    "        \"value\": 11.21,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 8.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"LoadAvg\"},\n",
    "            {\"kpiCategory\": \"CPU\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"INSTANCE\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"MySql\"},\n",
    "            {\"componentVersion\": \"8.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-1031-397-T-S-28335897\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"serviceId\": \"shopping-product-service\",\n",
    "        \"kpi\": \"RESPONSE_TIME\",\n",
    "        \"value\": 113.98,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"RESPONSE TIME\"},\n",
    "            {\"kpiCategory\": \"Workload\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"CLUSTER\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"Springboot Micro-service\"},\n",
    "            {\"javaVersion\": \"8.0\"}\n",
    "        ],\n",
    "        \"forensics\": \"CPU Utilization: 98.94%\n",
    "        Number of CPU cores: 8\n",
    "        Note: Total of CPU Utilization depends on number of CPU cores and can be > 100%\n",
    "        -----\n",
    "        PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n",
    "        11629 root      20   0   16160    236     20 R  94.1  0.0   4:27.17 batchjob\n",
    "        14602 heal      20   0 5119288 835492  16448 S  11.8  5.1 398:57.59 mysqld\n",
    "        \"\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-1031-397-T-S-28335899\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"serviceId\": \"shopping-product-service\",\n",
    "        \"kpi\": \"RESPONSE_TIME\",\n",
    "        \"value\": 117.92,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"RESPONSE TIME\"},\n",
    "            {\"kpiCategory\": \"Workload\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"CLUSTER\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"Springboot Micro-service\"},\n",
    "            {\"javaVersion\": \"8.0\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(HUGGING_FACE_GPTQ_REPO_NAME,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                          # torch_dtype=torch.bfloat16,\n",
    "                                                          # use_flash_attention_2=True,\n",
    "                                                          # low_cpu_mem_usage=True,\n",
    "                                                         )\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(HUGGING_FACE_GPTQ_REPO_NAME, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains. You answer specifically only to the question asked. \n",
      "<|user|>\n",
      "Analyse and provide root cause for the following ITOps incident. Use the forensics to support \n",
      "your answer. You should be technically accurate and up to the point.\n",
      "[\n",
      "    {\n",
      "        \"incidentId\": \"E-12-1-635-1700153820\",\n",
      "        \"incidentStartTime\": \"11/16/2023 16:57\",\n",
      "        \"probabilityScore\": 0.82,\n",
      "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335897\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"instanceId\": \"mysql-percona-153\",\n",
      "        \"serviceId\": \"mysql-percona-svc\",\n",
      "        \"kpi\": \"CPU_UTIL\",\n",
      "        \"value\": 98.44,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"CPU\"},\n",
      "            {\"kpiCategory\": \"CPU\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"MySql\"},\n",
      "            {\"componentVersion\": \"8.0\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335899\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:59\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"instanceId\": \"mysql-percona-153\",\n",
      "        \"serviceId\": \"mysql-percona-svc\",\n",
      "        \"kpi\": \"CPU_UTIL\",\n",
      "        \"value\": 99.2,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"CPU\"},\n",
      "            {\"kpiCategory\": \"CPU\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"MySql\"},\n",
      "            {\"componentVersion\": \"8.0\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-6224-6-C-S-ALL-28335898\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:58\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"instanceId\": \"mysql-percona-153\",\n",
      "        \"serviceId\": \"mysql-percona-svc\",\n",
      "        \"kpi\": \"LOAD_AVG\",\n",
      "        \"value\": 11.21,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 8.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"LoadAvg\"},\n",
      "            {\"kpiCategory\": \"CPU\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"MySql\"},\n",
      "            {\"componentVersion\": \"8.0\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-1031-397-T-S-28335897\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"serviceId\": \"shopping-product-service\",\n",
      "        \"kpi\": \"RESPONSE_TIME\",\n",
      "        \"value\": 113.98,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"RESPONSE TIME\"},\n",
      "            {\"kpiCategory\": \"Workload\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"CLUSTER\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"Springboot Micro-service\"},\n",
      "            {\"javaVersion\": \"8.0\"}\n",
      "        ],\n",
      "        \"forensics\": \"CPU Utilization: 98.94%\n",
      "        Number of CPU cores: 8\n",
      "        Note: Total of CPU Utilization depends on number of CPU cores and can be > 100%\n",
      "        -----\n",
      "        PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n",
      "        11629 root      20   0   16160    236     20 R  94.1  0.0   4:27.17 batchjob\n",
      "        14602 heal      20   0 5119288 835492  16448 S  11.8  5.1 398:57.59 mysqld\n",
      "        \"\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-1031-397-T-S-28335899\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"serviceId\": \"shopping-product-service\",\n",
      "        \"kpi\": \"RESPONSE_TIME\",\n",
      "        \"value\": 117.92,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"RESPONSE TIME\"},\n",
      "            {\"kpiCategory\": \"Workload\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"CLUSTER\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"Springboot Micro-service\"},\n",
      "            {\"javaVersion\": \"8.0\"}\n",
      "        ]\n",
      "    }\n",
      "]\n",
      " \n",
      "<|assistant|>\n",
      "Based on the provided forensic data, it's clear that the high CPU utilization of the MySQL server is causing a ripple effect throughout the system. The MySQL server, identified as mysql-percona-153, is consuming almost all available CPU resources, leaving very little for other processes to run effectively. This is evident from the fact that the CPU utilization is over 98% during both anomalies, and the load average is also unusually high at 11.21.\n",
      "\n",
      "As a result, the application running on top of this database, shopping-product-service, is experiencing significant performance degradation due to long response times. The response time anomalies are directly linked to the CPU issues on the MySQL server, with a difference of less than a second between the anomalies. It's worth mentioning that the response time anomaly of the first occurrence was detected at 16:57 and the second occurrence was detected at 16:59.\n",
      "\n",
      "To further confirm our suspicion, we can look into the process list of the MySQL server during these anomalies using the `PROCESSLIST` command. From the forensic data provided, we see a single user, `heal`, running a batch job at 16:57 (PID: 11629), which has been consuming nearly all available CPU resources. Additionally, the MySQL daemon, `mysqld`, is also consuming CPU resources, but not to the same extent.\n",
      "\n",
      "The high CPU usage of the MySQL server could be attributed to several factors such as heavy workload, inefficient queries, or insufficient resource allocation. However, given the severity of the issue and its impact on the system, it's recommended that immediate action be taken to address the issue.\n",
      "\n",
      "In order to alleviate the issue, some possible solutions include:\n",
      "\n",
      "1. Increasing the resource allocation of the MySQL server to ensure that it has enough CPU power to handle the current workload.\n",
      "2. Identifying and optimizing the queries running against the MySQL server to reduce their resource consumption.\n",
      "3. Implementing a caching solution to reduce the number of database queries being executed.\n",
      "4. Load balancing the database workload across multiple MySQL servers to distribute the resource usage more evenly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains. You answer specifically only to the question asked.\"},\n",
    "    {\"role\": \"user\", \"content\": question},]\n",
    "gen_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "gen_output = model.generate(input_ids=gen_input,\n",
    "                            max_new_tokens=512,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7,\n",
    "                            top_k=50,\n",
    "                            top_p=0.95,\n",
    "                            repetition_penalty=1.1)\n",
    "out = tokenizer.decode(gen_output[0], skip_special_tokens=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inference Using GGUF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoConfig\n",
    "config = AutoConfig.from_pretrained(r\"/home/paperspace/documents/OpsHarmonySentinel/model/files/ohs-7b-alpha_8q.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.max_seq_len = 4096\n",
    "# config.max_answer_len= 1024\n",
    "config.config.max_new_tokens = 2048\n",
    "config.config.context_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(r\"/home/paperspace/documents/OpsHarmonySentinel/model/files/ohs-7b-alpha_8q.gguf\", model_type='mistral', gpu_layers=0, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the incident and forensics, we can conclude that the root cause of this ITOps incident is CPU contention due to high CPU utilization by the batchjob process (pid 11629) running on the same server as MySQL database (mysql-percona-153). This has resulted in high load average values for MySQL server and degraded performance, leading to increased response times for requests handled by the Springboot microservice (shopping-product-service). The forensics also indicate that there is no resource contention or bottleneck at the application layer.\n",
      "To mitigate this issue, we can consider options such as spreading out batch jobs across multiple servers with more resources or allocating dedicated CPU cores to critical workloads such as database and microservices. We may also need to optimize the performance of our current server by reducing unnecessary tasks during the execution of the batchjob process or considering tuning parameter such as kernel setting.\n"
     ]
    }
   ],
   "source": [
    "print(llm(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpsHarmonySentinel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
