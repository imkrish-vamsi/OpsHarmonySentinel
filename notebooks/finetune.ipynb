{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum\n",
      "  Downloading optimum-1.14.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting auto-gptq\n",
      "  Downloading auto_gptq-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting coloredlogs (from optimum)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers>=4.26.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (4.36.0.dev0)\n",
      "Requirement already satisfied: torch>=1.9 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from optimum) (2.1.1)\n",
      "Requirement already satisfied: packaging in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from optimum) (23.2)\n",
      "Requirement already satisfied: numpy in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from optimum) (1.24.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from optimum) (0.19.4)\n",
      "Requirement already satisfied: datasets in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from optimum) (2.15.0)\n",
      "Requirement already satisfied: accelerate>=0.22.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from auto-gptq) (0.24.1)\n",
      "Collecting sentencepiece (from auto-gptq)\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Collecting gekko (from auto-gptq)\n",
      "  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from auto-gptq) (0.4.1)\n",
      "Requirement already satisfied: peft>=0.5.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from auto-gptq) (0.6.3.dev0)\n",
      "Requirement already satisfied: tqdm in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from auto-gptq) (4.66.1)\n",
      "Requirement already satisfied: psutil in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from accelerate>=0.22.0->auto-gptq) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from accelerate>=0.22.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch>=1.9->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (0.15.0)\n",
      "Collecting protobuf (from transformers[sentencepiece]>=4.26.0->optimum)\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from datasets->optimum) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from datasets->optimum) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from datasets->optimum) (3.9.1)\n",
      "Requirement already satisfied: six in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from aiohttp->datasets->optimum) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from jinja2->torch>=1.9->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Downloading optimum-1.14.1-py3-none-any.whl (399 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading auto_gptq-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, rouge, protobuf, humanfriendly, gekko, coloredlogs, optimum, auto-gptq\n",
      "Successfully installed auto-gptq-0.5.1 coloredlogs-15.0.1 gekko-1.0.6 humanfriendly-10.0 optimum-1.14.1 protobuf-4.25.1 rouge-1.0.1 sentencepiece-0.1.99\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Collecting matplotlib!=3.6.1,>=3.3 (from seaborn)\n",
      "  Downloading matplotlib-3.7.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Downloading fonttools-4.45.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.2/155.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\n",
      "Collecting pillow>=6.2.0 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Downloading Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib!=3.6.1,>=3.3->seaborn)\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.3->seaborn) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.45.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.45.1 importlib-resources-6.1.1 kiwisolver-1.4.5 matplotlib-3.7.4 pillow-10.1.0 pyparsing-3.1.1 seaborn-0.13.0\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting Click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from wandb) (68.0.0)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from wandb) (4.8.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from wandb) (4.25.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, Click, gitdb, GitPython, wandb\n",
      "Successfully installed Click-8.1.7 GitPython-3.1.40 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.3.6.tar.gz (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from flash-attn) (2.1.1)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from flash-attn) (23.2)\n",
      "Collecting ninja (from flash-attn)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: filelock in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from torch->flash-attn) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.3.6-cp38-cp38-linux_x86_64.whl size=56478132 sha256=c944e7f5a6ccd90c6a536ec81bf29d33806c9dec0cc9d2c13cbfe7404910796e\n",
      "  Stored in directory: /home/paperspace/.cache/pip/wheels/11/1b/a7/2d547c5254b47a3151b89feaeef981a0f12ffa1203096c9158\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: ninja, einops, flash-attn\n",
      "Successfully installed einops-0.7.0 flash-attn-2.3.6 ninja-1.11.1.1\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from scipy) (1.24.4)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipywidgets) (8.12.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipywidgets) (5.14.0)\n",
      "Collecting widgetsnbextension~=4.0.9 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.9 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/214.9 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, scipy, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.1 jupyterlab-widgets-3.0.9 scipy-1.10.1 widgetsnbextension-4.0.9\n"
     ]
    }
   ],
   "source": [
    "! pip install -q bitsandbytes datasets accelerate loralib\n",
    "! pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
    "! pip install optimum auto-gptq\n",
    "! pip install seaborn\n",
    "! pip install wandb\n",
    "! pip install flash-attn --no-build-isolation\n",
    "! pip install scipy ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891c853412074cdcbc73271aeed6d517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/paperspace/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimthegodfather\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/documents/OpsHarmonySentinel/notebooks/wandb/run-20231201_040203-xs86stt9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/imthegodfather/OpsHarmonySentinel/runs/xs86stt9' target=\"_blank\">stoic-flower-9</a></strong> to <a href='https://wandb.ai/imthegodfather/OpsHarmonySentinel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/imthegodfather/OpsHarmonySentinel' target=\"_blank\">https://wandb.ai/imthegodfather/OpsHarmonySentinel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/imthegodfather/OpsHarmonySentinel/runs/xs86stt9' target=\"_blank\">https://wandb.ai/imthegodfather/OpsHarmonySentinel/runs/xs86stt9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/imthegodfather/OpsHarmonySentinel/runs/xs86stt9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f06656895b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"OpsHarmonySentinel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_DIR_PATH = ''\n",
    "OUTPUT_PATH = os.path.join(PROJECT_DIR_PATH, 'model', 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "BASELINE_MODEL_NAME = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "PROJECT_NAME = 'OpsHarmonySentinel'\n",
    "HUGGING_FACE_REPO_NAME = f'imTheGodFather/{PROJECT_NAME}'\n",
    "HUGGING_FACE_MERGED_REPO_NAME = f'{HUGGING_FACE_REPO_NAME}'\n",
    "HUGGING_FACE_GPTQ_REPO_NAME = f'{HUGGING_FACE_REPO_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('birgermoell/open_assistant_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User: User: Where did the first transistor com...</td>\n",
       "      <td>{'source': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User: User: How can I sync my cell phone with ...</td>\n",
       "      <td>{'source': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User: User: Explain the concept of blockchain ...</td>\n",
       "      <td>{'source': ''}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            meta\n",
       "0  User: User: Where did the first transistor com...  {'source': ''}\n",
       "1  User: User: How can I sync my cell phone with ...  {'source': ''}\n",
       "2  User: User: Explain the concept of blockchain ...  {'source': ''}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = \"/home/paperspace/documents/OpsHarmonySentinel/data/files/train.csv\"\n",
    "df = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_dialogue(text):\n",
    "    return [sentence.replace('User:', '').replace('Chip:', '').strip() for sentence in text.split('Assistant:')]\n",
    "\n",
    "def dialogue_to_chat(dialogue):\n",
    "    out = [{'role': 'system', 'content': 'You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains.'}]\n",
    "    # for idx, message in enumerate(dialogue):\n",
    "    #     role = 'user' if idx%2==0 else 'assistant'\n",
    "    #     out.append({'role': role, 'content': message})\n",
    "    input = f\"Investigate and provide root cause for the following incident - {dialogue['incident']}\"\n",
    "    out.append({'role': 'user', 'content': input})\n",
    "    out.append({'role': 'assistant', 'content': dialogue['root_cause']})\n",
    "    return out\n",
    "\n",
    "def chat_to_input(chat):\n",
    "    return tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "def process_example(example):\n",
    "    # out = text_to_dialogue(example)\n",
    "    out = dialogue_to_chat(example)\n",
    "    out = chat_to_input(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: User: Does anyone have any tips on saving money?\n",
      "Assistant: Chip: One tip for saving money is to create a budget and stick to it. Make sure you track your expenses and put aside money for savings each month. Additionally, research deals and discounts online before you make a purchase, and take advantage of any coupons or promotions. You can also look for alternatives to expensive items, and consider cooking meals at home rather than eating out. Finally, try to pay down any debt and reduce frivolous spending.\n",
      "\n",
      "<|system|>\n",
      "You are a friendly chatbot assistant.</s>\n",
      "<|user|>\n",
      "Does anyone have any tips on saving money?</s>\n",
      "<|assistant|>\n",
      "One tip for saving money is to create a budget and stick to it. Make sure you track your expenses and put aside money for savings each month. Additionally, research deals and discounts online before you make a purchase, and take advantage of any coupons or promotions. You can also look for alternatives to expensive items, and consider cooking meals at home rather than eating out. Finally, try to pay down any debt and reduce frivolous spending.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = df['text'].iloc[12312]\n",
    "\n",
    "print(example)\n",
    "print()\n",
    "print(process_example(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [process_example(row) for idx, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains.</s>\n",
      "<|user|>\n",
      "Investigate and provide root cause for the following incident - [\n",
      "    {\n",
      "        \"incidentId\": \"E-99-9-999-9999999999\",\n",
      "        \"incidentStartTime\": \"04/01/2023 08:30\",\n",
      "        \"probabilityScore\": 0.90,\n",
      "        \"anomalyId\": \"AE-99-9999-9-F-S-ALL-99999999\",\n",
      "        \"anomalyTimestamp\": \"04/01/2023 08:30\",\n",
      "        \"applicationId\": \"affected-app\",\n",
      "        \"instanceId\": \"linux-server-42\",\n",
      "        \"serviceId\": \"affected-service\",\n",
      "        \"kpi\": \"INODES_USAGE\",\n",
      "        \"value\": 100.0,\n",
      "        \"thresholds\": {\"Upper\": 99.0, \"Lower\": 0.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"Inodes\"},\n",
      "            {\"kpiCategory\": \"FileSystem\"},\n",
      "            {\"kpiType\": \"Count\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"CRITICAL\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"Linux\"},\n",
      "            {\"componentName\": \"File System\"},\n",
      "            {\"componentType\": \"Inodes\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-99-9999-2-F-S-ALL-99999998\",\n",
      "        \"anomalyTimestamp\": \"04/01/2023 08:32\",\n",
      "        \"applicationId\": \"affected-app\",\n",
      "        \"instanceId\": \"linux-server-42\",\n",
      "        \"serviceId\": \"logging-service\",\n",
      "        \"kpi\": \"LOG_WRITE_FAILURES\",\n",
      "        \"value\": 50,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 0.0},\n",
      "        \"violationType\": \"Non-zero\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"WriteFailures\"},\n",
      "            {\"kpiCategory\": \"FileSystem\"},\n",
      "            {\"kpiType\": \"Count\"},\n",
      "            {\"anomalyLevel\": \"SERVICE\"},\n",
      "            {\"severity\": \"HIGH\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"Linux\"},\n",
      "            {\"componentName\": \"Logging Service\"},\n",
      "            {\"componentType\": \"Write Operations\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-99-9999-3-F-S-ALL-99999997\",\n",
      "        \"anomalyTimestamp\": \"04/01/2023 08:33\",\n",
      "        \"applicationId\": \"affected-app\",\n",
      "        \"instanceId\": \"linux-server-42\",\n",
      "        \"serviceId\": \"temp-file-creation-service\",\n",
      "        \"kpi\": \"TEMP_FILE_CREATION_FAILURES\",\n",
      "        \"value\": 30,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 0.0},\n",
      "        \"violationType\": \"Non-zero\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"FileCreationFailures\"},\n",
      "            {\"kpiCategory\": \"FileSystem\"},\n",
      "            {\"kpiType\": \"Count\"},\n",
      "            {\"anomalyLevel\": \"SERVICE\"},\n",
      "            {\"severity\": \"HIGH\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"Linux\"},\n",
      "            {\"componentName\": \"Temporary File Service\"},\n",
      "            {\"componentType\": \"File Creation\"}\n",
      "        ]\n",
      "    }\n",
      "]</s>\n",
      "<|assistant|>\n",
      "The root cause could be due to the Linux server running out of inodes, which has prevented new files from being created on the server, thereby causing failures in services that require writing logs or temporary files for normal operation.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(map(process_example, df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = list(map(tokenizer, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f2623be2820>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot sequence Lengths\n",
    "import seaborn as sns\n",
    "\n",
    "input_lengths = [len(x['input_ids']) for x in tokenized_data]\n",
    "sns.displot(input_lengths, kde=True).set(title='Distribution of Text Sequence Lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(.99 * len(data))\n",
    "train_data, val_data = tokenized_data[:split_idx], tokenized_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ee3e735ccb4fdb8e19eeb19f8b1fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2a4b8e1bcd4048995328c1ff264f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d26ae74ce4a439fb56eb67366f60c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7d1d798337456e81fd46d21d4fc71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1217053257b64534b48e2ffaa8faa024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7eb102cdda043ffbf0d3adc044b252e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdce2c51884b4f5e88d505a624a68d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80375308535341d695e6ca3f21165354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ae577d3c944645aad4c98df6162139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c750e8731064cfea112341321d8f287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4874b2792007463db43d4944deaf2cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842e5c75668444978687e2e1dd8fb4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a800d3e5fe184b10a33033ef4a7d8d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get quantized model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                          load_in_8bit=True,     # call for the 8 bit bnb quantized version\n",
    "                                                          device_map='auto'\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PEFT adapter config (16:32)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],                             # Apply to \"q_proj\", \"v_proj\" layers of attention (as suggested by paper)\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stabilize output layer and layernorms\n",
    "model = prepare_model_for_kbit_training(model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PEFT adapter on model (Last step)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "MAXLEN=512\n",
    "BATCH_SIZE=6\n",
    "GRAD_ACC=4\n",
    "WARMUP=100\n",
    "STEPS=50\n",
    "OPTIMIZER='paged_adamw_8bit' # save memory\n",
    "LR=4e-5                      # slightly smaller than pretraining lr | and close to LoRA standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Callbacks\n",
    "early_stop = transformers.EarlyStoppingCallback(10, 1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training config\n",
    "training_config = transformers.TrainingArguments(per_device_train_batch_size=BATCH_SIZE,\n",
    "                                                 gradient_accumulation_steps=GRAD_ACC,\n",
    "                                                 warmup_steps=WARMUP,\n",
    "                                                 max_steps=STEPS,\n",
    "                                                 optim=OPTIMIZER,\n",
    "                                                 learning_rate=LR,\n",
    "                                                 fp16=True,            # consider compatibility when using bf16\n",
    "                                                 logging_steps=1,\n",
    "                                                 output_dir=OUTPUT_PATH,\n",
    "                                                 report_to='wandb',\n",
    "                                                 # earlyStopping callback requirements\n",
    "                                                 load_best_model_at_end=True,\n",
    "                                                 evaluation_strategy='steps',\n",
    "                                                 metric_for_best_model='eval_loss',\n",
    "                                                 greater_is_better=False,\n",
    "                                                 eval_steps=10,\n",
    "                                                 save_steps=10,\n",
    "                                                 save_total_limit=2,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "trainer = transformers.Trainer(model=model,\n",
    "                               train_dataset=train_data,\n",
    "                               eval_dataset=val_data,\n",
    "                               data_collator=data_collator,\n",
    "                               args=training_config,\n",
    "                               callbacks=[early_stop],\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 44:17, Epoch 11/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.811600</td>\n",
       "      <td>0.900489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.853900</td>\n",
       "      <td>0.874403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.780600</td>\n",
       "      <td>0.829738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.764200</td>\n",
       "      <td>0.759371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.669697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.7833893740177155, metrics={'train_runtime': 2710.2188, 'train_samples_per_second': 0.443, 'train_steps_per_second': 0.018, 'total_flos': 6.879137197031424e+16, 'train_loss': 0.7833893740177155, 'epoch': 11.76})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_REPO_NAME = HUGGING_FACE_REPO_NAME + '_7B_alpha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imTheGodFather/OpsHarmonySentinel_7B_alpha'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(HUGGING_FACE_REPO_NAME)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Adapters to dequantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try Model with Lora adapter\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a friendly chatbot assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Hello, what are your limitations as a seven billion parameters nlp model ?\"},\n",
    "# ]\n",
    "#\n",
    "# gen_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to('cuda')\n",
    "# gen_output = model.generate(input_ids=gen_input, max_new_tokens=512, do_sample=True)\n",
    "# print(tokenizer.decode(gen_output[0], skip_special_tokens=True))\n",
    "# # timeit : 12.6 s ± 3.39 s per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peft config\n",
    "from peft import PeftConfig\n",
    "config = PeftConfig.from_pretrained(HUGGING_FACE_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e749a648514443b5a00a3275ec989479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get base model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                          torch_dtype=torch.float16, # GPTQ quantization requires fp16\n",
    "                                                          return_dict=True,\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lora model\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, HUGGING_FACE_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASELINE_MODEL_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_MERGED_REPO_NAME = HUGGING_FACE_MERGED_REPO_NAME + '_7B_merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imTheGodFather/OpsHarmonySentinel_7B_merged'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_MERGED_REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/paperspace/documents/OpsHarmonySentinel/notebooks/finetune.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B184.105.4.101/home/paperspace/documents/OpsHarmonySentinel/notebooks/finetune.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m merged_model\u001b[39m.\u001b[39;49mpush_to_hub(HUGGING_FACE_MERGED_REPO_NAME)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B184.105.4.101/home/paperspace/documents/OpsHarmonySentinel/notebooks/finetune.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mpush_to_hub(HUGGING_FACE_MERGED_REPO_NAME)\n",
      "File \u001b[0;32m~/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/transformers/utils/hub.py:871\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m files_timestamps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_files_timestamps(work_dir)\n\u001b[1;32m    870\u001b[0m \u001b[39m# Save all files.\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_pretrained(work_dir, max_shard_size\u001b[39m=\u001b[39;49mmax_shard_size, safe_serialization\u001b[39m=\u001b[39;49msafe_serialization)\n\u001b[1;32m    873\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upload_modified_files(\n\u001b[1;32m    874\u001b[0m     work_dir,\n\u001b[1;32m    875\u001b[0m     repo_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     commit_description\u001b[39m=\u001b[39mcommit_description,\n\u001b[1;32m    882\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/transformers/modeling_utils.py:2222\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[39mfor\u001b[39;00m shard_file, shard \u001b[39min\u001b[39;00m shards\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   2219\u001b[0m     \u001b[39mif\u001b[39;00m safe_serialization:\n\u001b[1;32m   2220\u001b[0m         \u001b[39m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[1;32m   2221\u001b[0m         \u001b[39m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[0;32m-> 2222\u001b[0m         safe_save_file(shard, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(save_directory, shard_file), metadata\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mformat\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m   2223\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2224\u001b[0m         save_function(shard, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[0;32m~/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/safetensors/torch.py:281\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_file\u001b[39m(\n\u001b[1;32m    251\u001b[0m     tensors: Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    252\u001b[0m     filename: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[1;32m    253\u001b[0m     metadata: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m ):\n\u001b[1;32m    255\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[39m=\u001b[39;49mmetadata)\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })"
     ]
    }
   ],
   "source": [
    "merged_model.push_to_hub(HUGGING_FACE_MERGED_REPO_NAME)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_MERGED_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize with GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFLOAD_PATH = os.path.join(PROJECT_DIR_PATH, 'offload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(HUGGING_FACE_REPO_NAME,\n",
    "                                                       padding_side='left',\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = transformers.GPTQConfig(bits=4,\n",
    "                                            #   group_size=128,\n",
    "                                            #   desc_act=False,\n",
    "                                            #  dataset=train_data,\n",
    "                                              dataset=data[:660],\n",
    "                                              tokenizer=tokenizer,\n",
    "                                              use_exllama=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a539a78e28ad41d6839723862456e510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e79b4076af432b9e098c795ed7aa47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc45a5be29f47e29e4be4e0057646b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1580798c1cc7473c94d5f368d0f6c155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1223a931dd44a0bb2d5eb57759f10a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b027c6d2bba947c791f64e161f844194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fea88e4280b4895953ecec73522831d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c2671874fc464b92e9539bd06501c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb91c987384d41f9862f4d21655b172d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c79b2a2ec4467a9b9b8d4b434f2e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33acc02e87a048be8a5ebe5ce431e051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be73c154360f4e55b6ac6f0f54934b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0acd0f2bb24eecbf48201fc0c7b671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829e37584c7743af8437576e3e4ef822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91da3eeccb644045943a898a5a253fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a46493a60847c5b15b386925de3929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c869f2fb87d408e8978196b61fc9354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d4e53688aa4bf5a67b3e47bb4d9bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d8cc69619a4fb6864f3a2f4d645656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dfb992cb0b4d64808e814565fc2b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008aba360aee4fff99d26f4276c1dc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9028bbcef1459a8e80bfd2fa5d7ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792e1fdb46254a30823a8659fcd176a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47543edf62a14029bc85464bacb36cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60df5d55b5d4b02bd79421d81aed3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd4c879619a45859d669dff92db75cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd297ab99bd3479fb0a6585c440fffec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ca9e28bd024c00a04c5c0173ef4d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134e5898bbd647ef9398ea769b92954c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad421878d984aac8c01b06bbde3ecb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73375f5d556f4a039469024142427432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4432a3e01f24d29a798941e9c0322f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669962a34729478585ed3467903bf48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6099a49c387446790f66b59bb8fd424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantize model\n",
    "q_model = transformers.AutoModelForCausalLM.from_pretrained(HUGGING_FACE_MERGED_REPO_NAME,\n",
    "                                                            quantization_config=quantization_config,\n",
    "                                                            # torch_dtype=torch.float16,\n",
    "                                                            device_map=\"auto\",\n",
    "                                                            # offload_folder=OFFLOAD_PATH\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_GPTQ_REPO_NAME = HUGGING_FACE_GPTQ_REPO_NAME + '_7B_alpha_4Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imTheGodFather/OpsHarmonySentinel_7B_alpha_4Q'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGING_FACE_GPTQ_REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20de43dfdecb4b4dbb15e694dec3db1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/imTheGodfather/OpsHarmonySentinel_7B_alpha_4Q/commit/4e19fac090e02c3704a3c744d6fdc00c23e51efc', commit_message='Upload tokenizer', commit_description='', oid='4e19fac090e02c3704a3c744d6fdc00c23e51efc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to HF hub\n",
    "q_model.push_to_hub(HUGGING_FACE_GPTQ_REPO_NAME)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_GPTQ_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36db4b373bab49bb82dcf43e5eba4cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/348k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73b4d9bbff14103a9ac538bea861ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(HUGGING_FACE_GPTQ_REPO_NAME, device_map=\"auto\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(HUGGING_FACE_GPTQ_REPO_NAME, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(HUGGING_FACE_MERGED_REPO_NAME,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                          torch_dtype=torch.bfloat16,\n",
    "                                                          use_flash_attention_2=True,\n",
    "#                                                          low_cpu_mem_usage=True,\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(HUGGING_FACE_GPTQ_REPO_NAME,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                          # torch_dtype=torch.bfloat16,\n",
    "                                                          use_flash_attention_2=True,\n",
    "                                                        #  low_cpu_mem_usage=True,\n",
    "                                                        # token=\"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\"\n",
    "                                                         )\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(HUGGING_FACE_GPTQ_REPO_NAME, padding_side='left', \n",
    "                                                      #  token=\"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\"\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Analyse and provide root cause for the following ITOps incident - [\n",
    "    {\n",
    "        \"incidentId\": \"E-12-2-636-1700153821\",\n",
    "        \"incidentStartTime\": \"11/17/2023 09:23\",\n",
    "        \"probabilityScore\": 0.75,\n",
    "        \"anomalyId\": \"AE-12-7458-2-C-LB-ALL-28335900\",\n",
    "        \"anomalyTimestamp\": \"11/17/2023 09:23\",\n",
    "        \"applicationId\": \"customer-portal-app\",\n",
    "        \"serviceId\": \"web-server\",\n",
    "        \"kpi\": \"REQUEST_COUNT\",\n",
    "        \"value\": 1200,\n",
    "        \"thresholds\": {\"Upper\": 0, \"Lower\": 1000},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"REQUEST_COUNT\"},\n",
    "            {\"kpiCategory\": \"Traffic\"},\n",
    "            {\"anomalyLevel\": \"CLUSTER\"},\n",
    "            {\"severity\": \"MODERATE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"componentName\": \"Load Balancer\"},\n",
    "            {\"componentVersion\": \"3.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-7458-2-C-LB-ALL-28335901\",\n",
    "        \"anomalyTimestamp\": \"11/17/2023 09:25\",\n",
    "        \"applicationId\": \"customer-portal-app\",\n",
    "        \"serviceId\": \"web-server\",\n",
    "        \"kpi\": \"RESPONSE_TIME\",\n",
    "        \"value\": 350,\n",
    "        \"thresholds\": {\"Upper\": 0, \"Lower\": 250},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"RESPONSE_TIME\"},\n",
    "            {\"kpiCategory\": \"Performance\"},\n",
    "            {\"anomalyLevel\": \"CLUSTER\"},\n",
    "            {\"severity\": \"MODERATE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"componentName\": \"Load Balancer\"},\n",
    "            {\"componentVersion\": \"3.0\"}\n",
    "        ]\n",
    "    }\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Analyse and provide root cause for the following ITOps incident. Use the forensics to support \n",
    "your answer. You should be technically accurate and up to the point.\n",
    "[\n",
    "    {\n",
    "        \"incidentId\": \"E-12-1-635-1700153820\",\n",
    "        \"incidentStartTime\": \"11/16/2023 16:57\",\n",
    "        \"probabilityScore\": 0.82,\n",
    "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335897\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"instanceId\": \"mysql-percona-153\",\n",
    "        \"serviceId\": \"mysql-percona-svc\",\n",
    "        \"kpi\": \"CPU_UTIL\",\n",
    "        \"value\": 98.44,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"CPU\"},\n",
    "            {\"kpiCategory\": \"CPU\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"INSTANCE\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"MySql\"},\n",
    "            {\"componentVersion\": \"8.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335899\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:59\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"instanceId\": \"mysql-percona-153\",\n",
    "        \"serviceId\": \"mysql-percona-svc\",\n",
    "        \"kpi\": \"CPU_UTIL\",\n",
    "        \"value\": 99.2,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"CPU\"},\n",
    "            {\"kpiCategory\": \"CPU\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"INSTANCE\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"MySql\"},\n",
    "            {\"componentVersion\": \"8.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-6224-6-C-S-ALL-28335898\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:58\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"instanceId\": \"mysql-percona-153\",\n",
    "        \"serviceId\": \"mysql-percona-svc\",\n",
    "        \"kpi\": \"LOAD_AVG\",\n",
    "        \"value\": 11.21,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 8.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"LoadAvg\"},\n",
    "            {\"kpiCategory\": \"CPU\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"INSTANCE\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"MySql\"},\n",
    "            {\"componentVersion\": \"8.0\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-1031-397-T-S-28335897\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"serviceId\": \"shopping-product-service\",\n",
    "        \"kpi\": \"RESPONSE_TIME\",\n",
    "        \"value\": 113.98,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"RESPONSE TIME\"},\n",
    "            {\"kpiCategory\": \"Workload\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"CLUSTER\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"Springboot Micro-service\"},\n",
    "            {\"javaVersion\": \"8.0\"}\n",
    "        ],\n",
    "        \"forensics\": \"CPU Utilization: 98.94%\n",
    "        Number of CPU cores: 8\n",
    "        Note: Total of CPU Utilization depends on number of CPU cores and can be > 100%\n",
    "        -----\n",
    "        PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n",
    "        11629 root      20   0   16160    236     20 R  94.1  0.0   4:27.17 batchjob\n",
    "        14602 heal      20   0 5119288 835492  16448 S  11.8  5.1 398:57.59 mysqld\n",
    "        \"\n",
    "    },\n",
    "    {\n",
    "        \"anomalyId\": \"AE-12-1031-397-T-S-28335899\",\n",
    "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
    "        \"applicationId\": \"shopping-cart-app\",\n",
    "        \"serviceId\": \"shopping-product-service\",\n",
    "        \"kpi\": \"RESPONSE_TIME\",\n",
    "        \"value\": 117.92,\n",
    "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
    "        \"violationType\": \"Greater Than\",\n",
    "        \"tags\": [\n",
    "            {\"kpi\": \"RESPONSE TIME\"},\n",
    "            {\"kpiCategory\": \"Workload\"},\n",
    "            {\"kpiType\": \"Core\"},\n",
    "            {\"anomalyLevel\": \"CLUSTER\"},\n",
    "            {\"severity\": \"SEVERE\"}\n",
    "        ],\n",
    "        \"components\": [\n",
    "            {\"operatingSystem\": \"RHEL_8\"},\n",
    "            {\"componentName\": \"Springboot Micro-service\"},\n",
    "            {\"javaVersion\": \"8.0\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cbbfba2e954639a592b2ef23f871ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# base model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/OpsHarmonySentinel/lib/python3.8/site-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains. You answer specifically only to the question asked. \n",
      "<|user|>\n",
      "Analyse and provide root cause for the following ITOps incident. Use the forensics to support \n",
      "your answer. You should be technically accurate and up to the point.\n",
      "[\n",
      "    {\n",
      "        \"incidentId\": \"E-12-1-635-1700153820\",\n",
      "        \"incidentStartTime\": \"11/16/2023 16:57\",\n",
      "        \"probabilityScore\": 0.82,\n",
      "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335897\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"instanceId\": \"mysql-percona-153\",\n",
      "        \"serviceId\": \"mysql-percona-svc\",\n",
      "        \"kpi\": \"CPU_UTIL\",\n",
      "        \"value\": 98.44,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"CPU\"},\n",
      "            {\"kpiCategory\": \"CPU\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"MySql\"},\n",
      "            {\"componentVersion\": \"8.0\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-6224-1-C-S-ALL-28335899\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:59\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"instanceId\": \"mysql-percona-153\",\n",
      "        \"serviceId\": \"mysql-percona-svc\",\n",
      "        \"kpi\": \"CPU_UTIL\",\n",
      "        \"value\": 99.2,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 85.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"CPU\"},\n",
      "            {\"kpiCategory\": \"CPU\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"MySql\"},\n",
      "            {\"componentVersion\": \"8.0\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-6224-6-C-S-ALL-28335898\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:58\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"instanceId\": \"mysql-percona-153\",\n",
      "        \"serviceId\": \"mysql-percona-svc\",\n",
      "        \"kpi\": \"LOAD_AVG\",\n",
      "        \"value\": 11.21,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 8.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"LoadAvg\"},\n",
      "            {\"kpiCategory\": \"CPU\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"INSTANCE\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"MySql\"},\n",
      "            {\"componentVersion\": \"8.0\"}\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-1031-397-T-S-28335897\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"serviceId\": \"shopping-product-service\",\n",
      "        \"kpi\": \"RESPONSE_TIME\",\n",
      "        \"value\": 113.98,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"RESPONSE TIME\"},\n",
      "            {\"kpiCategory\": \"Workload\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"CLUSTER\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"Springboot Micro-service\"},\n",
      "            {\"javaVersion\": \"8.0\"}\n",
      "        ],\n",
      "        \"forensics\": \"CPU Utilization: 98.94%\n",
      "        Number of CPU cores: 8\n",
      "        Note: Total of CPU Utilization depends on number of CPU cores and can be > 100%\n",
      "        -----\n",
      "        PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n",
      "        11629 root      20   0   16160    236     20 R  94.1  0.0   4:27.17 batchjob\n",
      "        14602 heal      20   0 5119288 835492  16448 S  11.8  5.1 398:57.59 mysqld\n",
      "        \"\n",
      "    },\n",
      "    {\n",
      "        \"anomalyId\": \"AE-12-1031-397-T-S-28335899\",\n",
      "        \"anomalyTimestamp\": \"11/16/2023 16:57\",\n",
      "        \"applicationId\": \"shopping-cart-app\",\n",
      "        \"serviceId\": \"shopping-product-service\",\n",
      "        \"kpi\": \"RESPONSE_TIME\",\n",
      "        \"value\": 117.92,\n",
      "        \"thresholds\": {\"Upper\": 0.0, \"Lower\": 100.0},\n",
      "        \"violationType\": \"Greater Than\",\n",
      "        \"tags\": [\n",
      "            {\"kpi\": \"RESPONSE TIME\"},\n",
      "            {\"kpiCategory\": \"Workload\"},\n",
      "            {\"kpiType\": \"Core\"},\n",
      "            {\"anomalyLevel\": \"CLUSTER\"},\n",
      "            {\"severity\": \"SEVERE\"}\n",
      "        ],\n",
      "        \"components\": [\n",
      "            {\"operatingSystem\": \"RHEL_8\"},\n",
      "            {\"componentName\": \"Springboot Micro-service\"},\n",
      "            {\"javaVersion\": \"8.0\"}\n",
      "        ]\n",
      "    }\n",
      "]\n",
      " \n",
      "<|assistant|>\n",
      "Based on the provided forensic data, it's clear that the high CPU utilization of the MySQL server is causing a ripple effect throughout the system. The MySQL server, identified as mysql-percona-153, is consuming almost all available CPU resources, leaving very little for other processes to run effectively. This is evident from the fact that the CPU utilization is over 98% during both anomalies, and the load average is also unusually high at 11.21.\n",
      "\n",
      "As a result, the application running on top of this database, shopping-product-service, is experiencing significant performance degradation due to long response times. The response time anomalies are directly linked to the CPU issues on the MySQL server, with a difference of less than a second between the anomalies. It's worth mentioning that the response time anomaly of the first occurrence was detected at 16:57 and the second occurrence was detected at 16:59.\n",
      "\n",
      "To further confirm our suspicion, we can look into the process list of the MySQL server during these anomalies using the `PROCESSLIST` command. From the forensic data provided, we see a single user, `heal`, running a batch job at 16:57 (PID: 11629), which has been consuming nearly all available CPU resources. Additionally, the MySQL daemon, `mysqld`, is also consuming CPU resources, but not to the same extent.\n",
      "\n",
      "The high CPU usage of the MySQL server could be attributed to several factors such as heavy workload, inefficient queries, or insufficient resource allocation. However, given the severity of the issue and its impact on the system, it's recommended that immediate action be taken to address the issue.\n",
      "\n",
      "In order to alleviate the issue, some possible solutions include:\n",
      "\n",
      "1. Increasing the resource allocation of the MySQL server to ensure that it has enough CPU power to handle the current workload.\n",
      "2. Identifying and optimizing the queries running against the MySQL server to reduce their resource consumption.\n",
      "3. Implementing a caching solution to reduce the number of database queries being executed.\n",
      "4. Load balancing the database workload across multiple MySQL servers to distribute the resource usage more evenly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot assistant made by HEAL. You are an expert in ITOps, servers, applications and all other related domains. You answer specifically only to the question asked.\"},\n",
    "    {\"role\": \"user\", \"content\": question},]\n",
    "gen_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "gen_output = model.generate(input_ids=gen_input,\n",
    "                            max_new_tokens=512,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7,\n",
    "                            top_k=50,\n",
    "                            top_p=0.95,\n",
    "                            repetition_penalty=1.1)\n",
    "out = tokenizer.decode(gen_output[0], skip_special_tokens=True)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpsHarmonySentinel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
